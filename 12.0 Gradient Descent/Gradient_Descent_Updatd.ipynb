{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Python notebook we will go through an example of implementing **Gradient Descent** in simple and multiple linear regression, for this we will be using housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "import pandas as pd\n",
    "housing = pd.read_csv('Housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Yes to 1 and No to 0\n",
    "housing['mainroad'] = housing['mainroad'].map({'yes': 1, 'no': 0})\n",
    "housing['guestroom'] = housing['guestroom'].map({'yes': 1, 'no': 0})\n",
    "housing['basement'] = housing['basement'].map({'yes': 1, 'no': 0})\n",
    "housing['hotwaterheating'] = housing['hotwaterheating'].map({'yes': 1, 'no': 0})\n",
    "housing['airconditioning'] = housing['airconditioning'].map({'yes': 1, 'no': 0})\n",
    "housing['prefarea'] = housing['prefarea'].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting furnishingstatus column to binary column using get_dummies\n",
    "status = pd.get_dummies(housing['furnishingstatus'],drop_first=True)\n",
    "housing = pd.concat([housing,status],axis=1)\n",
    "housing.drop(['furnishingstatus'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisisng the data\n",
    "housing = (housing - housing.mean())/housing.std()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression\n",
    "# Assign feature variable X\n",
    "X = housing['area']\n",
    "\n",
    "# Assign response variable to y\n",
    "y = housing['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional way to import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# To visualise in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the relationship between the features and the response using scatterplots\n",
    "sns.pairplot(housing, x_vars='area', y_vars='price',size=7, aspect=0.7, kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For linear regression we use a cost function known as the mean squared error or MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gd1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply partial derivative with respect to m and c and will equate it to zero to find the least value of m and c for which our cost function get the lowest value as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gd2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to apply gradient descent from scratch we need our X and y variables as numpy arrays, Let's convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent function\n",
    "# Takes in X, y, current m and c (both initialised to 0), num_iterations, learning rate\n",
    "# returns gradient at current m and c for each pair of m and c\n",
    "\n",
    "def gradient(X, y, m_current=0, c_current=0, iters=1000, learning_rate=0.01):\n",
    "    N = float(len(y))\n",
    "    gd_df = pd.DataFrame( columns = ['m_current', 'c_current','cost'])\n",
    "    for i in range(iters):\n",
    "        y_current = (m_current * X) + c_current\n",
    "        cost = sum([data**2 for data in (y-y_current)]) / N\n",
    "        m_gradient = -(2/N) * sum(X * (y - y_current))\n",
    "        c_gradient = -(2/N) * sum(y - y_current)\n",
    "        m_current = m_current - (learning_rate * m_gradient)\n",
    "        c_current = c_current - (learning_rate * c_gradient)\n",
    "        gd_df.loc[i] = [m_current,c_current,cost]\n",
    "    return(gd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print gradients at multiple (m, c) pairs\n",
    "# notice that gradient decreased gradually towards 0\n",
    "# we have used 1000 iterations, can use more if needed\n",
    "gradients = gradient(X,y)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting cost against num_iterations\n",
    "gradients.reset_index().plot.line(x='index', y=['cost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Regression: Applying Gradient Descent for Multiple (>1) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning feature variable X\n",
    "X = housing[['area','bedrooms']]\n",
    "\n",
    "# Assigning response variable y\n",
    "y = housing['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a columns of 1s as an intercept to X.\n",
    "# The intercept column is needed for convenient matrix representation of cost function\n",
    "\n",
    "X['intercept'] = 1\n",
    "X = X.reindex_axis(['intercept','area','bedrooms'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y to arrays\n",
    "import numpy as np\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theta is the vector representing coefficients (intercept, area, bedrooms)\n",
    "theta = np.matrix(np.array([0,0,0])) \n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function\n",
    "# takes in theta (current values of coefficients b0, b1, b2), X and y\n",
    "# returns total cost at current b0, b1, b2\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    return np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on [Numpy Matmul](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gd.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# takes in current X, y, learning rate alpha, num_iters\n",
    "# returns cost (notice it uses the cost function defined above)\n",
    "\n",
    "def gradient_descent_multi(X, y, theta, alpha, iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(X)\n",
    "    gdm_df = pd.DataFrame( columns = ['Bets','cost'])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n",
    "        theta = theta - alpha * gradient\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        gdm_df.loc[i] = [theta,cost]\n",
    "\n",
    "    return gdm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print costs with various values of coefficients b0, b1, b2\n",
    "gradient_descent_multi(X, y, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cost\n",
    "gradient_descent_multi(X, y, theta, alpha, iterations).reset_index().plot.line(x='index', y=['cost'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CareerUpSkill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
